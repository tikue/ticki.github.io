<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Algorithms on Ticki</title>
    <link>/tags/algorithms/</link>
    <description>Recent content in Algorithms on Ticki</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 23 Oct 2016 23:25:15 +0200</lastBuildDate>
    <atom:link href="/tags/algorithms/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>How LZ4 works</title>
      <link>/blog/how-lz4-works/</link>
      <pubDate>Sun, 23 Oct 2016 23:25:15 +0200</pubDate>
      
      <guid>/blog/how-lz4-works/</guid>
      <description>&lt;script type=&#34;text/javascript&#34;
  src=&#34;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&#34;&gt;
&lt;/script&gt;

&lt;p&gt;LZ4 is a really fast compression algorithm with a reasonable compression ratio, but unfortunately there is limited documentation on how it works. The only explanation (not spec, explanation) &lt;a href=&#34;https://fastcompression.blogspot.com/2011/05/lz4-explained.html&#34;&gt;can be found&lt;/a&gt; on the author&#39;s blog, but I think it is less of an explanation and more of an informal specification.&lt;/p&gt;

&lt;p&gt;This blog post tries to explain it such that anybody (even new beginners) can understand and implement it.&lt;/p&gt;

&lt;h1 id=&#34;linear-smallinteger-code-lsic&#34;&gt;Linear small-integer code (LSIC)&lt;/h1&gt;

&lt;p&gt;The first part of LZ4 we need to explain is a smart but simple integer encoder. It is very space efficient for 0-255, and then grows linearly, based on the assumption that the integers used with this encoding rarely exceeds this limit, as such it is only used for small integers in the standard.&lt;/p&gt;

&lt;p&gt;It is a form of addition code, in which we read a byte. If this byte is the maximal value (255), another byte is read and added to the sum. This process is repeated until a byte below 255 is reached, which will be added to the sum, and the sequence will then end.&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;/img/lz4_int_encoding_flowchart.svg&#34; alt=&#34;We try to fit it into the next cluster.&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;In short, we just keep adding bytes and stop when we hit a non-0xFF byte.&lt;/p&gt;

&lt;p&gt;We&#39;ll use the name &amp;quot;LSIC&amp;quot; for convinience.&lt;/p&gt;

&lt;h1 id=&#34;block&#34;&gt;Block&lt;/h1&gt;

&lt;p&gt;An LZ4 stream is divided into segments called &amp;quot;blocks&amp;quot;. Blocks contains a literal which is to be copied directly to the output stream, and then a back reference, which tells us to copy some number of bytes from the already decompressed stream.&lt;/p&gt;

&lt;p&gt;This is really were the compression is going on. Copying from the old stream allows deduplication and runs-length encoding.&lt;/p&gt;

&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;

&lt;p&gt;A block looks like:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\overbrace{\underbrace{t_1}_\text{4 bits}\  \underbrace{t_2}_\text{4 bits}}^\text{Token} \quad \underbrace{\overbrace{e_1}^\texttt{LISC}}_\text{If $t_1 = 15$} \quad \underbrace{\overbrace{L}^\text{Literal}}_{t_1 + e\text{ bytes }} \quad \overbrace{\underbrace{O}_\text{2 bytes}}^\text{Little endian} \quad \underbrace{\overbrace{e_2}^\texttt{LISC}}_\text{If $t_2 = 15$}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;And decodes to the &lt;span  class=&#34;math&#34;&gt;\(L\)&lt;/span&gt; segment, followed by a &lt;span  class=&#34;math&#34;&gt;\(t_2 + e_2 + 4\)&lt;/span&gt; bytes sequence copied from position &lt;span  class=&#34;math&#34;&gt;\(l - O\)&lt;/span&gt; from the output buffer (where &lt;span  class=&#34;math&#34;&gt;\(l\)&lt;/span&gt; is the length of the output buffer).&lt;/p&gt;

&lt;p&gt;We will explain all of these in the next sections.&lt;/p&gt;

&lt;h2 id=&#34;token&#34;&gt;Token&lt;/h2&gt;

&lt;p&gt;Any block starts with a 1 byte token, which is divided into two 4-bit fields.&lt;/p&gt;

&lt;h2 id=&#34;literals&#34;&gt;Literals&lt;/h2&gt;

&lt;p&gt;The first (highest) field in the token is used to define the literal. This obviously takes a value 0-15.&lt;/p&gt;

&lt;p&gt;Since we might want to encode higher integer, as such we make use of LSIC encoding: If the field is 15 (the meximal value), we read an integer with LSIC and add it to the original value (15) to obtain the literals length.&lt;/p&gt;

&lt;p&gt;Call the final value &lt;span  class=&#34;math&#34;&gt;\(L\)&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;Then we forward the next &lt;span  class=&#34;math&#34;&gt;\(L\)&lt;/span&gt; bytes from the input stream to the output stream.&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;/img/lz4_literals_copy_diagram.svg&#34; alt=&#34;We copy from the buffer directly.&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;h2 id=&#34;deduplication&#34;&gt;Deduplication&lt;/h2&gt;

&lt;p&gt;The next few bytes are used to define some segment in the already decoded buffer, which is going to be appended to the output buffer.&lt;/p&gt;

&lt;p&gt;This allows us to transmit a position and a length to read from in the already decoded buffer instead of transmitting the literals themself.&lt;/p&gt;

&lt;p&gt;To start with, we read a 16-bit little endian integer. This defines the so called offset, &lt;span  class=&#34;math&#34;&gt;\(O\)&lt;/span&gt;. It is important to understand that the offset is not the starting position of the copied buffer. This starting point is calculated by &lt;span  class=&#34;math&#34;&gt;\(l - O\)&lt;/span&gt; with &lt;span  class=&#34;math&#34;&gt;\(l\)&lt;/span&gt; being the number of bytes already decoded.&lt;/p&gt;

&lt;p&gt;Secondly, similarly to the literals length, if &lt;span  class=&#34;math&#34;&gt;\(t_2\)&lt;/span&gt; is 15 (the maximal value), we use LSIC to &amp;quot;extend&amp;quot; this value and we add the result. This plus 4 yields the number of bytes we will copy from the output buffer. The reason we add 4 is because copying less than 4 bytes would result in a negative expansion of the compressed buffer.&lt;/p&gt;

&lt;p&gt;Now that we know the start position and the length, we can append the segment to the buffer itself:&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;/img/lz4_deduplicating_diagram.svg&#34; alt=&#34;Copying in action.&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;It is important to understand that the end of the segment might not be initializied before the rest of the segment is appended, because overlaps are allowed. This allows a neat trick, namely &amp;quot;runs-length encoding&amp;quot;, where you repeat some sequence a given number of times:&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;/img/lz4_runs_encoding_diagram.svg&#34; alt=&#34;We repeat the last byte.&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;Note that the duplicate section is not required if you&#39;re in the end of the stream, i.e. if there&#39;s no more compressed bytes to read.&lt;/p&gt;

&lt;h1 id=&#34;compression&#34;&gt;Compression&lt;/h1&gt;

&lt;p&gt;Until now, we have only considered decoding, not the reverse process.&lt;/p&gt;

&lt;p&gt;A dozen of approaches to compression exists. They have the aspects that they need to be able to find duplicates in the already input buffer.&lt;/p&gt;

&lt;p&gt;In general, there are two classes of such compression algorithms:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;HC: High-compression ratio algorithms, these are often very complex, and might include steps like backtracking, removing repeatation, non-greediy.&lt;/li&gt;
&lt;li&gt;FC: Fast compression, these are simpler and faster, but provides a slightly worse compression ratio.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We will focus on the FC-class algorithms.&lt;/p&gt;

&lt;p&gt;Binary Search Trees (often B-trees) are often used for searching for duplicates. In particular, every byte iterated over will add a pointer to the rest of the buffer to a B-tree, we call the &amp;quot;duplicate tree&amp;quot;. Now, B-trees allows us to retrieve the largest element smaller than or equal to some key. In lexiographic ordering, this is equivalent to asking the element sharing the largest number of bytes as prefix.&lt;/p&gt;

&lt;p&gt;For example, consider the table:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;abcdddd =&amp;gt; 0
bcdddd  =&amp;gt; 1
cdddd   =&amp;gt; 2
dddd    =&amp;gt; 3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If we search for &lt;code&gt;cddda&lt;/code&gt;, we&#39;ll get a partial match, namely &lt;code&gt;cdddd =&amp;gt; 2&lt;/code&gt;. So we can quickly find out how many bytes they have in common as prefix. In this case, it is 4 bytes.&lt;/p&gt;

&lt;p&gt;What if we found no match or a bad match (a match that shares less than some threshold)? Well, then we write it as literal until a good match is found.&lt;/p&gt;

&lt;p&gt;As you may notice, the dictionary grows linearly. As such, it is important that you reduce memory once in a while, by trimming it. Note that just trimming the first (or last) &lt;span  class=&#34;math&#34;&gt;\(N\)&lt;/span&gt; entries is inefficient, because some might be used often. Instead, a &lt;a href=&#34;https://en.wikipedia.org/wiki/Cache_Replacement_Policies&#34;&gt;cache replacement policy&lt;/a&gt; should be used. If the dictionary is filled, the cache replacement policy should determine which match should be replaced. I&#39;ve found PLRU a good choice of CRP for LZ4 compression.&lt;/p&gt;

&lt;p&gt;Note that you should add additional rules like being addressible (within &lt;span  class=&#34;math&#34;&gt;\(2^{16} + 4\)&lt;/span&gt; bytes of the cursor, which is required because &lt;span  class=&#34;math&#34;&gt;\(O\)&lt;/span&gt; is 16-bit) and being above some length (smaller keys have worse block-level compression ratio).&lt;/p&gt;

&lt;p&gt;Another faster but worse (compression-wise) approach is hashing every four bytes and placing them in a table. This means that you can only look up the latest sequence given some 4-byte prefix. Looking up allows you to progress and see how long the duplicate sequence match. When you can&#39;t go any longer, you encode the literals section until another duplicate 4-byte is found.&lt;/p&gt;

&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;LZ4 is a reasonably simple algorithm with reasonably good compression ratio. It is the type of algorithm that you can implement on an afternoon without much complication.&lt;/p&gt;

&lt;p&gt;If you need a portable and efficient compression algorithm which can be implement in only a few hundreds of lines, LZ4 would be my go-to.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>On Random-Access Compression</title>
      <link>/blog/on-random-access-compression/</link>
      <pubDate>Sun, 23 Oct 2016 23:25:15 +0200</pubDate>
      
      <guid>/blog/on-random-access-compression/</guid>
      <description>&lt;script type=&#34;text/javascript&#34;
  src=&#34;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&#34;&gt;
&lt;/script&gt;

&lt;p&gt;This post will contains an algorithm I came up with, doing efficient rolling compression. It&#39;s going to be used in &lt;a href=&#34;https://github.com/ticki/tfs&#34;&gt;TFS&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&#34;what-is-rolling-compression&#34;&gt;What is rolling compression?&lt;/h1&gt;

&lt;p&gt;Consider that you have a large file and you want to compress it. That&#39;s easy enough and many algorithms exists for doing so. Now, consider that you want to read or write a small part of the file.&lt;/p&gt;

&lt;p&gt;Most algorithms would require you to decompress, write, and recompress the whole file. Clearly, this gets expensive when the file is big.&lt;/p&gt;

&lt;h1 id=&#34;clusterbased-compression&#34;&gt;Cluster-based compression&lt;/h1&gt;

&lt;p&gt;A cluster is some small fixed-size block (often 512, 1024, or 4096 bytes). We can have a basic cluster allocator by linking unused clusters together. Cluster-centric compression is interesting, because it can exploit the allocator.&lt;/p&gt;

&lt;p&gt;So, the outline is that we compress every &lt;span  class=&#34;math&#34;&gt;\(n\)&lt;/span&gt; adjacent clusters to some &lt;span  class=&#34;math&#34;&gt;\(n&#39; &lt; n%&gt;\)&lt;/span&gt;, then we can free the excessive clusters in this compressed line.&lt;/p&gt;

&lt;h1 id=&#34;copyonwrite&#34;&gt;Copy-on-write&lt;/h1&gt;

&lt;p&gt;Our algorithm is not writable, but it can be written by allocating, copying, and deallocating. This is called copy-on-write, or COW for short. It is a common technique used in many file systems.&lt;/p&gt;

&lt;p&gt;Essentially, we never write a cluster. Instead, we allocate a new cluster, and copy the data to it. Then we deallocate the old cluster.&lt;/p&gt;

&lt;p&gt;This allows us to approach everything much more functionally, and we thus don&#39;t have to worry about make compressible blocks uncompressible (consider that you overwrite a highly compressible cluster with random data, then you extend a physical cluster containing many virtual clusters, these wouldn&#39;t be possible to have in one cluster).&lt;/p&gt;

&lt;h1 id=&#34;physical-and-virtual-clusters&#34;&gt;Physical and virtual clusters&lt;/h1&gt;

&lt;p&gt;Our goal is really fit multiple clusters into one physical cluster. Therefore, it is essential to distinguish between physical (the stored) and virtual (the compressed) clusters.&lt;/p&gt;

&lt;p&gt;A physical cluster can contain up to 8 virtual clusters. A pointer to a virtual cluster starts with 3 bits defining the index into the physical cluster, which is defined by the rest of the pointer.&lt;/p&gt;

&lt;p&gt;The allocated physical cluster contains 8 bitflags, defining which of the 8 virtual clusters in the physical cluster are used. This allows us to know how many virtual clusters we need to go over before we get the target decompressed cluster.&lt;/p&gt;

&lt;p&gt;When the integer hits zero (i.e. all the virtual clusters are freed), the physical cluster is freed.&lt;/p&gt;

&lt;p&gt;Since an active cluster will never have the state zero, we use this blind state to represent an uncompressed physical cluster. This means we maximally have one byte in space overhead for uncompressible clusters.&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;/img/virtual_physical_random_access_compression_diagram.svg&#34; alt=&#34;A diagram&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;h1 id=&#34;the-physical-cluster-allocator&#34;&gt;The physical cluster allocator&lt;/h1&gt;

&lt;p&gt;The cluster allocator is nothing but a linked list of clusters. Every free cluster links to another free cluster or NIL (no more free clusters).&lt;/p&gt;

&lt;p&gt;This method is called SLOB (Simple List Of Objects) and has the advantage of being complete zero-cost in that there is no wasted space.&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;/img/slob_allocation_diagram.svg&#34; alt=&#34;Physical allocation is simply linked list of free objects.&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;h1 id=&#34;the-virtual-cluster-allocator&#34;&gt;The virtual cluster allocator&lt;/h1&gt;

&lt;p&gt;Now we hit the meat of the matter.&lt;/p&gt;

&lt;p&gt;When virtual cluster is allocated, we read from the physical cluster list. The first thing we will check is if we can fit in our virtual cluster into the cluster next to the head of the list (we wrap if we reach the end).&lt;/p&gt;

&lt;p&gt;If we can fit it in &lt;em&gt;and&lt;/em&gt; we have less than 8 virtual clusters in this physical cluster, we will put it into the compressed physical cluster at the first free virtual slot (and then set the respective bitflag):&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;/img/allocating_compressed_virtual_page_into_next_diagram.svg&#34; alt=&#34;We try to fit it into the next cluster.&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;If we cannot, we pop the list and use the fully-free physical cluster to store etablish a new stack of virtual clusters. It starts as uncompressed:&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;/img/pop_and_create_new_uncompressed_cluster_diagram.svg&#34; alt=&#34;We pop the list and put the virtual cluster in the physical uncompressed slot.&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;h1 id=&#34;properties-of-this-approach&#34;&gt;Properties of this approach&lt;/h1&gt;

&lt;p&gt;This approach to writable random-access compression has some very nice properties.&lt;/p&gt;

&lt;h2 id=&#34;compression-miss&#34;&gt;Compression miss&lt;/h2&gt;

&lt;p&gt;We call it a compression miss when we need to pop from the freelist (i.e. we cannot fit it into the cluster next to the head). When you allocate you can maximally have one compression miss, and therefore allocation is constant-time.&lt;/p&gt;

&lt;h2 id=&#34;every-cluster-has-a-sister-cluster&#34;&gt;Every cluster has a sister cluster&lt;/h2&gt;

&lt;p&gt;Because the &amp;quot;next cluster or wrap&amp;quot; function is bijective, we&#39;re sure that we try to insert a virtual cluster to every cluster at least once. This wouldn&#39;t be true if we used a hash function or something else.&lt;/p&gt;

&lt;p&gt;This has the interesting consequence that filled clusters won&#39;t be tried to allocate in multiple times.&lt;/p&gt;

&lt;h1 id=&#34;limitations&#34;&gt;Limitations&lt;/h1&gt;

&lt;p&gt;A number of limitations are in this algorithms. The first and most obvious one is the limitation on the compression ratio. This is a minor one: it limits the ratio to maxmially slightly less than 1:8.&lt;/p&gt;

&lt;p&gt;A more important limitation is fragmentation. If I allocate many clusters and then deallocate some of them such that many adjacent physical clusters only contain one virtual cluster, this row will have a compression ratio of 1:1 until they&#39;re deallocated. Note that it is very rare that this happens, and will only marginally affect the global compression ratio.&lt;/p&gt;

&lt;h1 id=&#34;update-an-idea&#34;&gt;Update: An idea&lt;/h1&gt;

&lt;p&gt;A simple trick can improve performance in some cases. Instead of compressing all the virtual clusters in a physical cluster together, you should compress each virtual cluster seperately and place them sequentially (with some delimiter) in the physical cluster.&lt;/p&gt;

&lt;p&gt;If your compression algorithm is streaming, you can much faster iterate to the right delimiter, and then only decompress that virtual cluster.&lt;/p&gt;

&lt;p&gt;This has the downside of making the compression ratio worse. One solution is to have an initial dictionary (if using a dictionary-based compression algorithm).&lt;/p&gt;

&lt;p&gt;Another idea is to eliminate the cluster state and replace it by repeated delimiters. I need to investigate this some more with benchmarks and so on in order to tell if this is actually superior to having a centralized cluster state.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Skip Lists: Done Right</title>
      <link>/blog/skip-lists-done-right/</link>
      <pubDate>Sat, 17 Sep 2016 13:46:49 +0200</pubDate>
      
      <guid>/blog/skip-lists-done-right/</guid>
      <description>

&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.css&#34;&gt;&lt;/p&gt;

&lt;h1 id=&#34;what-is-a-skip-list&#34;&gt;What is a skip list?&lt;/h1&gt;

&lt;p&gt;In short, skip lists are a linked-list-like structure which allows for fast search. It consists of a base list holding the elements, together with a tower of lists maintaining a linked hierarchy of subsequences, each skipping over fewer elements.&lt;/p&gt;

&lt;p&gt;Skip list is a wonderful data structure, one of my personal favorites, but a trend in the past ten years has made them more and more uncommon as a single-threaded in-memory structure.&lt;/p&gt;

&lt;p&gt;My take is that this is because of how hard they are to get right. The simplicity can easily fool you into being too relaxed with respect to performance, and while they are simple, it is important to pay attention to the details.&lt;/p&gt;

&lt;p&gt;In the past five years, people have become increasingly sceptical of skip lists&amp;rsquo; performance, due to their poor cache behavior when compared to e.g. B-trees, but fear not, a good implementation of skip lists can easily outperform B-trees while being implementable in only a couple of hundred lines.&lt;/p&gt;

&lt;p&gt;How? We will walk through a variety of techniques that can be used to achieve this speed-up.&lt;/p&gt;

&lt;p&gt;These are my thoughts on how a bad and a good implementation of skip list looks like.&lt;/p&gt;

&lt;h2 id=&#34;advantages&#34;&gt;Advantages&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Skip lists perform very well on rapid insertions because there are no rotations or reallocations.&lt;/li&gt;
&lt;li&gt;They&amp;rsquo;re simpler to implement than both self-balancing binary search trees and hash tables.&lt;/li&gt;
&lt;li&gt;You can retrieve the next element in constant time (compare to logarithmic time for inorder traversal for BSTs and linear time in hash tables).&lt;/li&gt;
&lt;li&gt;The algorithms can easily be modified to a more specialized structure (like segment or range &amp;ldquo;trees&amp;rdquo;, indexable skip lists, or keyed priority queues).&lt;/li&gt;
&lt;li&gt;Making it lockless is simple.&lt;/li&gt;
&lt;li&gt;It does well in persistent (slow) storage (often even better than AVL and EH).&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;a-naïve-but-common-implementation&#34;&gt;A naïve (but common) implementation&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/nNjOtfa.png&#34; alt=&#34;Each shortcut has its own node.&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Our skip list consists of (in this case, three) lists, stacked such that the &lt;b style=&#34;font: 400 1.21em KaTeX_Math&#34;&gt;n&lt;/b&gt;&amp;lsquo;th list visits a subset of the node the &lt;b style=&#34;font: 400 1.21em KaTeX_Math&#34;&gt;n - 1&lt;/b&gt;&amp;lsquo;th list does. This subset is defined by a probability distribution, which we will get back to later.&lt;/p&gt;

&lt;p&gt;If you rotate the skip list and remove duplicate edges, you can see how it resembles a binary search tree:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/DO031ek.png&#34; alt=&#34;A binary search tree.&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Say I wanted to look up the node &amp;ldquo;30&amp;rdquo;, then I&amp;rsquo;d perform normal binary search from the root and down. Due to duplicate nodes, we use the rule of going right if both children are equal:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/H5KjvqC.png&#34; alt=&#34;Searching the tree.&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Self-balancing Binary Search Trees often have complex algorithms to keep the tree balanced, but skip lists are easier: They aren&amp;rsquo;t trees, they&amp;rsquo;re similar to trees in some ways, but they are not trees.&lt;/p&gt;

&lt;p&gt;Every node in the skip list is given a &amp;ldquo;height&amp;rdquo;, defined by the highest level containing the node (similarly, the number of decendants of a leaf containing the same value). As an example, in the above diagram, &amp;ldquo;42&amp;rdquo; has height 2, &amp;ldquo;25&amp;rdquo; has height 3, and &amp;ldquo;11&amp;rdquo; has height 1.&lt;/p&gt;

&lt;p&gt;When we insert, we assign the node a height, following the probability distribution:&lt;/p&gt;

&lt;p&gt;&lt;center style=&#34;font: 400 1.21em KaTeX_Math;font-style: italic;&#34;&gt; p(n) = 2&lt;sup&gt;1-n&lt;/sup&gt; &lt;/center&gt;&lt;/p&gt;

&lt;p&gt;To obtain this distribution, we flip a coin until it hits tails, and count the flips:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;uint generate_level() {
    uint n = 0;
    while coin_flip() {
        n++;
    }

    return n;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;By this distribution, statistically the parent layer would contain half as many nodes, so searching is amortized &lt;b style=&#34;font: 400 1.21em KaTeX_Main&#34;&gt;O(log &lt;i&gt;n&lt;/i&gt;) &lt;/b&gt;.&lt;/p&gt;

&lt;p&gt;Note that we only have pointers to the right and below node, so insertion must be done while searching, that is, instead of searching and then inserting, we insert whenever we go a level down (pseudocode):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;-- Recursive skip list insertion function.
define insert(elem, root, height, level):
    if right of root &amp;lt; elem:
        -- If right isn&#39;t &amp;quot;overshot&amp;quot; (i.e. we are going to long), we go right.
        return insert(elem, right of root, height, level)
    else:
        if level = 0:
            -- We&#39;re at bottom level and the right node is overshot, hence
            -- we&#39;ve reached our goal, so we insert the node inbetween root
            -- and the node next to root.
            old ← right of root
            right of root ← elem
            right of elem ← old
        else:
            if level ≤ height:
                -- Our level is below the height, hence we need to insert a
                -- link before we go on.
                old ← right of root
                right of root ← elem
                right of elem ← old

            -- Go a level down.
            return insert(elem, below root, height, level - 1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The above algorithm is recursive, but we can with relative ease turn it into an iterative form (or let tail-call optimization do the job for us).&lt;/p&gt;

&lt;p&gt;As an example, here&amp;rsquo;s a diagram, the curved lines marks overshoots/edges where a new node is inserted:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/jr9V8Ot.png&#34; alt=&#34;An example&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;waste-waste-everywhere&#34;&gt;Waste, waste everywhere&lt;/h1&gt;

&lt;p&gt;That seems fine doesn&amp;rsquo;t it? No, not at all. It&amp;rsquo;s absolute garbage.&lt;/p&gt;

&lt;p&gt;There is a total and complete waste of space going on. Let&amp;rsquo;s assume there are &lt;b style=&#34;font: 400 1.21em KaTeX_Math&#34;&gt;n&lt;/b&gt; elements, then the tallest node is approximately &lt;b style=&#34;font: 400 1.21em KaTeX_Main&#34;&gt;&lt;i&gt;h = &lt;/i&gt;log&lt;sub&gt;2&lt;/sub&gt; &lt;i&gt;n&lt;/i&gt;&lt;/b&gt;, that gives us approximately &lt;b style=&#34;font: 400 1.21em KaTeX_Main&#34;&gt;1 + Σ&lt;sub&gt;&lt;i&gt;k ←0..h&lt;/i&gt;&lt;/sub&gt; &lt;i&gt;&lt;/i&gt;2&lt;sup&gt;&lt;i&gt;-k&lt;/i&gt;&lt;/sup&gt; n ≈ 2&lt;i&gt;n&lt;/i&gt;&lt;/b&gt;.&lt;/p&gt;

&lt;p&gt;&lt;b style=&#34;font: 400 1.21em KaTeX_Math&#34;&gt;2&lt;i&gt;n&lt;/i&gt;&lt;/b&gt; is certainly no small amount, especially if you consider what each node contains, a pointer to the inner data, the node right and down, giving 5 pointers in total, so a single structure of &lt;b style=&#34;font: 400 1.21em KaTeX_Math&#34;&gt;&lt;i&gt;n&lt;/i&gt;&lt;/b&gt; nodes consists of approximately &lt;b style=&#34;font: 400 1.21em KaTeX_Math&#34;&gt;6&lt;i&gt;n&lt;/i&gt;&lt;/b&gt; pointers.&lt;/p&gt;

&lt;p&gt;But memory isn&amp;rsquo;t even the main concern! When you need to follow a pointer on every decrease (apprx. 50% of all the links), possibly leading to cache misses. It turns out that there is a really simple fix for solving this:&lt;/p&gt;

&lt;p&gt;Instead of linking vertically, a good implementation should consist of a singly linked list, in which each node contains  an array (representing the nodes above) with pointers to later nodes:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/Fd6gDLv.png&#34; alt=&#34;A better skip list.&#34; /&gt;&lt;/p&gt;

&lt;p&gt;If you represent the links (&amp;ldquo;shortcuts&amp;rdquo;) through dynamic arrays, you will still often get cache miss. Particularly, you might get a cache miss on both the node itself (which is not data local) and/or the dynamic array. As such, I recommend using a fixed-size array (beware of the two negative downsides: 1. more space usage, 2. a hard limit on the highest level, and the implication of linear upperbound when &lt;i style=&#34;font: 400 1.21em KaTeX_Math&#34;&gt;h &amp;gt; c&lt;/i&gt;. Furthermore, you should keep small enough to fit a cache line.).&lt;/p&gt;

&lt;p&gt;Searching is done by following the top shortcuts as long as you don&amp;rsquo;t overshoot your target, then you decrement the level and repeat, until you reach the lowest level and overshoot. Here&amp;rsquo;s an example of searching for &amp;ldquo;22&amp;rdquo;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/cQsPnGa.png&#34; alt=&#34;Searching for &amp;quot;22&amp;quot;.&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In pseudocode:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;define search(skip_list, needle):
    -- Initialize to the first node at the highest level.
    level ← max_level
    current_node ← root of skip_list

    loop:
        -- Go right until we overshoot.
        while level&#39;th shortcut of current_node &amp;lt; needle:
            current_node ← level&#39;th shortcut of current_node

        if level = 0:
            -- We hit our target.
            return current_node
        else:
            -- Decrement the level.
            level ← level - 1
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;b-style-font-400-1-21em-katex-math-o-1-b-level-generation&#34;&gt;&lt;b style=&#34;font: 400 1.21em KaTeX_Math&#34;&gt;O(1)&lt;/b&gt; level generation&lt;/h1&gt;

&lt;p&gt;Even William Pugh did this mistake in &lt;a href=&#34;http://epaperpress.com/sortsearch/download/skiplist.pdf&#34;&gt;his original paper&lt;/a&gt;. The problem lies in the way the level is generated: Repeating coin flips (calling the random number generator, and checking parity), can mean a couple of RNG state updates (approximately 2 on every insertion). If your RNG is a slow one (e.g. you need high security against DOS attacks), this is noticable.&lt;/p&gt;

&lt;p&gt;The output of the RNG is uniformly distributed, so you need to apply some function which can transform this into the desired distribution. My favorite is this one:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;define generate_level():
    -- First we apply some mask which makes sure that we don&#39;t get a level
    -- above our desired level. Then we find the first set bit.
    ffz(random() &amp;amp; ((1 &amp;lt;&amp;lt; max_level) - 1))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This of course implies that you &lt;code&gt;max_level&lt;/code&gt; is no higher than the bit width of the &lt;code&gt;random()&lt;/code&gt; output. In practice, most RNGs return 32-bit or 64-bit integers, which means this shouldn&amp;rsquo;t be a problem, unless you have more elements than there can be in your address space.&lt;/p&gt;

&lt;h1 id=&#34;improving-cache-efficiency&#34;&gt;Improving cache efficiency&lt;/h1&gt;

&lt;p&gt;A couple of techniques can be used to improve the cache efficiency:&lt;/p&gt;

&lt;h2 id=&#34;memory-pools&#34;&gt;Memory pools&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/Wa8IVBJ.png&#34; alt=&#34;A skip list in a memory pool.&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Our nodes are simply fixed-size blocks, so we can keep them data local, with high allocation/deallocation performance, through linked memory pools (SLOBs), which is basically just a list of free objects.&lt;/p&gt;

&lt;p&gt;The order doesn&amp;rsquo;t matter. Indeed, if we swap &amp;ldquo;9&amp;rdquo; and &amp;ldquo;7&amp;rdquo;, we can suddenly see that this is simply a skip list:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/O863RR1.png&#34; alt=&#34;It&#39;s true.&#34; /&gt;&lt;/p&gt;

&lt;p&gt;We can keep these together in some arbitrary number of (not necessarily consecutive) pages, drastically reducing cache misses, when the nodes are of smaller size.&lt;/p&gt;

&lt;p&gt;Since these are pointers into memory, and not indexes in an array, we need not reallocate on growth. We can simply extend the free list.&lt;/p&gt;

&lt;h2 id=&#34;flat-arrays&#34;&gt;Flat arrays&lt;/h2&gt;

&lt;p&gt;If we are interested in compactness and have a insertion/removal ratio near to 1, a variant of linked memory pools can be used: We can store the skip list in a flat array, such that we have indexes into said array instead of pointers.&lt;/p&gt;

&lt;h2 id=&#34;unrolled-lists&#34;&gt;Unrolled lists&lt;/h2&gt;

&lt;p&gt;Unrolled lists means that instead of linking each element, you link some number of fixed-size chuncks contains two or more elements (often the chunk is around 64 bytes, i.e. the normal cache line size).&lt;/p&gt;

&lt;p&gt;Unrolling is essential for a good cache performance. Depending on the size of the objects you store, unrolling can reduce cache misses when following links while searching by 50-80%.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s an example of an unrolled skip list:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/FYpPQPh.png&#34; alt=&#34;A simple 4 layer unrolled skip list.&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The gray box marks excessive space in the chunk, i.e. where new elements can be placed. Searching is done over the skip list, and when a candidate is found, the chunk is searched through &lt;strong&gt;linear&lt;/strong&gt; search. To insert, you push to the chunk (i.e. replace the first free space). If no excessive space is available, the insertion happens in the skip list itself.&lt;/p&gt;

&lt;p&gt;Note that these algorithms requires information about how we found the chunk. Hence we store a &amp;ldquo;back look&amp;rdquo;, an array of the last node visited, for each level. We can then backtrack if we couldn&amp;rsquo;t fit the element into the chunk.&lt;/p&gt;

&lt;p&gt;We effectively reduce cache misses by some factor depending on the size of the object you store. This is due to fewer links need to be followed before the goal is reached.&lt;/p&gt;

&lt;h1 id=&#34;self-balancing-skip-lists&#34;&gt;Self-balancing skip lists&lt;/h1&gt;

&lt;p&gt;Various techniques can be used to improve the height generation, to give a better distribution. In other words, we make the level generator aware of our nodes, instead of purely random, independent RNGs.&lt;/p&gt;

&lt;h2 id=&#34;self-correcting-skip-list&#34;&gt;Self-correcting skip list&lt;/h2&gt;

&lt;p&gt;The simplest way to achieve a content-aware level generator is to keep track of the number of node of each level in the skip list. If we assume there are &lt;b style=&#34;font: 400 1.21em KaTeX_Math&#34;&gt;&lt;i&gt;n&lt;/i&gt;&lt;/b&gt; nodes, the expected number of nodes with level &lt;b style=&#34;font: 400 1.21em KaTeX_Math&#34;&gt;&lt;i&gt;l&lt;/i&gt;&lt;/b&gt; is &lt;b style=&#34;font: 400 1.21em KaTeX_Main&#34;&gt;2&lt;sup&gt;&lt;i&gt;-l&lt;/i&gt;&lt;/sup&gt;&lt;i&gt;n&lt;/i&gt;&lt;/b&gt;. Subtracting this from actual number gives us a measure of how well-balanced each height is:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://i.imgur.com/bBf7kcg.png&#34; alt=&#34;Balance&#34; /&gt;&lt;/p&gt;

&lt;p&gt;When we generate a new node&amp;rsquo;s level, you choose one of the heights with the biggest under-representation (see the black line in the diagram), either randomly or by some fixed rule (e.g. the highest or the lowest).&lt;/p&gt;

&lt;h2 id=&#34;perfectly-balanced-skip-lists&#34;&gt;Perfectly balanced skip lists&lt;/h2&gt;

&lt;p&gt;Perfect balancing often ends up hurting performance, due to backwards level changes, but it is possible. The basic idea is to reduce the most over-represented level when removing elements.&lt;/p&gt;

&lt;h1 id=&#34;an-extra-remark&#34;&gt;An extra remark&lt;/h1&gt;

&lt;p&gt;Skip lists are wonderful as an alternative to Distributed Hash Tables. Performance is mostly about the same, but skip lists are more DoS resistant if you make sure that all links are F2F.&lt;/p&gt;

&lt;p&gt;Each node represents a node in the network. Instead of having a head node and a nil node, we connect the ends, so any machine can search starting at it self:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/moD7oy9.png&#34; alt=&#34;A network organized as a skip list.&#34; /&gt;&lt;/p&gt;

&lt;p&gt;If you want a secure open system, the trick is that any node can invite a node, giving it a level equal to or lower than the level itself. If the node control the key space in the interval of A to B, we partition it into two and transfer all KV pairs in the second part to the new node. Obviously, this approach has no privilege escalation, so you can&amp;rsquo;t initialize a sybil attack easily.&lt;/p&gt;

&lt;h1 id=&#34;conclusion-and-final-words&#34;&gt;Conclusion and final words&lt;/h1&gt;

&lt;p&gt;By apply a lot of small, subtle tricks, we can drastically improve performance of skip lists, providing a simpler and faster alternative to Binary Search Trees. Many of these are really just minor tweaks, but give an absolutely enormous speed-up.&lt;/p&gt;

&lt;p&gt;The diagrams were made with &lt;a href=&#34;https://en.wikipedia.org/wiki/Dia_(software)&#34;&gt;Dia&lt;/a&gt; and &lt;a href=&#34;https://en.wikipedia.org/wiki/PGF/TikZ&#34;&gt;TikZ&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>